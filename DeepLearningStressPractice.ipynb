{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+cfKDntfizidUmvJcFjaS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaptainOdin/stress/blob/main/DeepLearningStressPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### P1 Load/Explore"
      ],
      "metadata": {
        "id": "JgmlCparD4A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CaptainOdin/stress.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XibCth6NE9zJ",
        "outputId": "2e812ff1-6876-4cf5-f3d8-7f1991b4a557"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'stress' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "drEtFTTwFnCY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stress = pd.read_csv('/content/stress/Stressv2.csv')\n",
        "stress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PwbmRNbiEAol",
        "outputId": "f88abf25-6d04-4283-b2fa-1215b0f61f31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             subreddit post_id sentence_range  \\\n",
              "0                 ptsd  8601tu       (15, 20)   \n",
              "1           assistance  8lbrx9         (0, 5)   \n",
              "2                 ptsd  9ch1zh       (15, 20)   \n",
              "3        relationships  7rorpp        [5, 10]   \n",
              "4     survivorsofabuse  9p2gbc         [0, 5]   \n",
              "...                ...     ...            ...   \n",
              "2833     relationships  7oee1t       [35, 40]   \n",
              "2834              ptsd  9p4ung       [20, 25]   \n",
              "2835           anxiety  9nam6l        (5, 10)   \n",
              "2836    almosthomeless  5y53ya        [5, 10]   \n",
              "2837              ptsd  5y25cl         [0, 5]   \n",
              "\n",
              "                                                   text  label  confidence  \\\n",
              "0     He said he had not felt that way before, sugge...      1    0.800000   \n",
              "1     Hey there r/assistance, Not sure if this is th...      0    1.000000   \n",
              "2     My mom then hit me with the newspaper and it s...      1    0.800000   \n",
              "3     until i met my new boyfriend, he is amazing, h...      1    0.600000   \n",
              "4     October is Domestic Violence Awareness Month a...      1    0.800000   \n",
              "...                                                 ...    ...         ...   \n",
              "2833  * Her, a week ago: Precious, how are you? (I i...      0    1.000000   \n",
              "2834  I don't have the ability to cope with it anymo...      1    1.000000   \n",
              "2835  In case this is the first time you're reading ...      0    1.000000   \n",
              "2836  Do you find this normal? They have a good rela...      0    0.571429   \n",
              "2837  I was talking to my mom this morning and she s...      1    0.571429   \n",
              "\n",
              "      social_timestamp  \n",
              "0           1521614353  \n",
              "1           1527009817  \n",
              "2           1535935605  \n",
              "3           1516429555  \n",
              "4           1539809005  \n",
              "...                ...  \n",
              "2833        1515187044  \n",
              "2834        1539827412  \n",
              "2835        1539269312  \n",
              "2836        1488938143  \n",
              "2837        1488909516  \n",
              "\n",
              "[2838 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ee55886-dc1f-4bba-a304-8cf6743c2ba2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>post_id</th>\n",
              "      <th>sentence_range</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>confidence</th>\n",
              "      <th>social_timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>8601tu</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>He said he had not felt that way before, sugge...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1521614353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assistance</td>\n",
              "      <td>8lbrx9</td>\n",
              "      <td>(0, 5)</td>\n",
              "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1527009817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>9ch1zh</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>My mom then hit me with the newspaper and it s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1535935605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relationships</td>\n",
              "      <td>7rorpp</td>\n",
              "      <td>[5, 10]</td>\n",
              "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1516429555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>survivorsofabuse</td>\n",
              "      <td>9p2gbc</td>\n",
              "      <td>[0, 5]</td>\n",
              "      <td>October is Domestic Violence Awareness Month a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1539809005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2833</th>\n",
              "      <td>relationships</td>\n",
              "      <td>7oee1t</td>\n",
              "      <td>[35, 40]</td>\n",
              "      <td>* Her, a week ago: Precious, how are you? (I i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1515187044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2834</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>9p4ung</td>\n",
              "      <td>[20, 25]</td>\n",
              "      <td>I don't have the ability to cope with it anymo...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1539827412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2835</th>\n",
              "      <td>anxiety</td>\n",
              "      <td>9nam6l</td>\n",
              "      <td>(5, 10)</td>\n",
              "      <td>In case this is the first time you're reading ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1539269312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>almosthomeless</td>\n",
              "      <td>5y53ya</td>\n",
              "      <td>[5, 10]</td>\n",
              "      <td>Do you find this normal? They have a good rela...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1488938143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2837</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>5y25cl</td>\n",
              "      <td>[0, 5]</td>\n",
              "      <td>I was talking to my mom this morning and she s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1488909516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2838 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ee55886-dc1f-4bba-a304-8cf6743c2ba2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ee55886-dc1f-4bba-a304-8cf6743c2ba2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ee55886-dc1f-4bba-a304-8cf6743c2ba2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b5035070-e05b-4ce4-aeba-82e865d1b972\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5035070-e05b-4ce4-aeba-82e865d1b972')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b5035070-e05b-4ce4-aeba-82e865d1b972 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMfNCrdJFlwD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM and BERT\n",
        "##### First preprocess text"
      ],
      "metadata": {
        "id": "VDBkg8iZGEEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the text and labels from the dataset, use NLTK\n",
        "texts = stress['text'].values\n",
        "labels = stress['label'].values\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer and the list of stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = ''.join(c for c in text if 0 < ord(c) < 127)\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Lemmatize the words and filter out stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean the text data\n",
        "cleaned_texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ne9W-pYGbSE",
        "outputId": "d9cd342a-05af-4125-e21d-764ad1dcb8f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Print a Sample\n",
        "sample_size = 5\n",
        "for i in range(sample_size):\n",
        "    print(\"Original Text:\", texts[i])\n",
        "    print(\"Cleaned Text:\", cleaned_texts[i])\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 2. Check for Stopwords\n",
        "for i in range(sample_size):\n",
        "    remaining_stopwords = [word for word in cleaned_texts[i].split() if word in stop_words]\n",
        "    print(f\"Cleaned Text {i+1} Remaining Stopwords:\", remaining_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XbRjhupG0Ia",
        "outputId": "bbbf8203-c080-4b23-8058-756ad251ddf7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: He said he had not felt that way before, suggeted I go rest and so ..TRIGGER AHEAD IF YOUI'RE A HYPOCONDRIAC LIKE ME: i decide to look up \"feelings of doom\" in hopes of maybe getting sucked into some rabbit hole of ludicrous conspiracy, a stupid \"are you psychic\" test or new age b.s., something I could even laugh at down the road. No, I ended up reading that this sense of doom can be indicative of various health ailments; one of which I am prone to.. So on top of my \"doom\" to my gloom..I am now f'n worried about my heart. I do happen to have a physical in 48 hours.\n",
            "Cleaned Text: said felt way suggeted go rest trigger ahead youi hypocondriac like decide look feeling doom hope maybe getting sucked rabbit hole ludicrous conspiracy stupid psychic test new age b something could even laugh road ended reading sense doom indicative various health ailment one prone top doom gloom f n worried heart happen physical hour\n",
            "--------------------------------------------------\n",
            "Original Text: Hey there r/assistance, Not sure if this is the right place to post this.. but here goes =) I'm currently a student intern at Sandia National Labs and working on a survey to help improve our marketing outreach efforts at the many schools we recruit at around the country. We're looking for current undergrad/grad STEM students so if you're a STEM student or know STEM students, I would greatly appreciate if you can help take or pass along this short survey. As a thank you, everyone who helps take the survey will be entered in to a drawing for chance to win one of three $50 Amazon gcs.\n",
            "Cleaned Text: hey r assistance sure right place post go currently student intern sandia national lab working survey help improve marketing outreach effort many school recruit around country looking current undergrad grad stem student stem student know stem student would greatly appreciate help take pas along short survey thank everyone help take survey entered drawing chance win one three amazon gc\n",
            "--------------------------------------------------\n",
            "Original Text: My mom then hit me with the newspaper and it shocked me that she would do this, she knows I don't like play hitting, smacking, striking, hitting or violence of any sort on my person. Do I send out this vibe asking for it from the universe? Then yesterday I decided to take my friend to go help another \"friend\" move to a new place. While we were driving the friend we are moving strikes me on my shoulder. And I address it immediately because this is the 4th time I have told him not to do these things, then my other friend who is driving nearly gets into an collision with another car i think because he was high on marijuana and the friend we are moving in the backseat is like \"you have to understand I was just trying to get your attention\" you know the thing 5 year olds do to get peoples attention by smacking them, this guy is in his 60's.\n",
            "Cleaned Text: mom hit newspaper shocked would know like play hitting smacking striking hitting violence sort person send vibe asking universe yesterday decided take friend go help another friend move new place driving friend moving strike shoulder address immediately th time told thing friend driving nearly get collision another car think high marijuana friend moving backseat like understand trying get attention know thing year old get people attention smacking guy\n",
            "--------------------------------------------------\n",
            "Original Text: until i met my new boyfriend, he is amazing, he is kind, he is sweet, he is a good student, he likes the same things as me, my family likes him, and so on... but i dont feel that passion that rush i felt with my ex, the truth is that when i started going out with my boyfriend i secretly saw my ex a few times to see if i really didnt feel nothing for him, but it was disgusting, i didnt even want him to touch me, i feel bad with myself i didnt want him, but still, i was there. then i kinda realized i felt nothing love related for him and it was ok, HE was HURT when he knew i was dating this boy and he even begged me to stay but of course not. but now the problem is that when im with my boyfriend i dont feel like i love him, like that thing you kinda have to feel with a new love, i just feel \"ok\" with him, and i catch myself thinking about my ex from time to time, remembering all the good things we had and it drives me crazy because i know that if i see him again i wont feel that way, that \"love\" that my mind makes me think stills there. and recently i found out that he has a girl and he is actually enjoying the experience and i got so mad and so hurt (i know i dont have any right to feel that way) i felt betrayed and I STILL feel that way, i gross myself out.\n",
            "Cleaned Text: met new boyfriend amazing kind sweet good student like thing family like dont feel passion rush felt ex truth started going boyfriend secretly saw ex time see really didnt feel nothing disgusting didnt even want touch feel bad didnt want still kinda realized felt nothing love related ok hurt knew dating boy even begged stay course problem im boyfriend dont feel like love like thing kinda feel new love feel ok catch thinking ex time time remembering good thing drive crazy know see wont feel way love mind make think still recently found girl actually enjoying experience got mad hurt know dont right feel way felt betrayed still feel way gross\n",
            "--------------------------------------------------\n",
            "Original Text: October is Domestic Violence Awareness Month and I am a domestic violence survivor who is still struggling, even after over four years. Lately I have been feeling very angry. Angry that my abusive ex received no real consequences for his actions. This man abused me in all manners: physically, sexually, emotionally, verbally, financially, etc. I was granted a restraining order against him (and it was renewed a year later) but I was unable to press criminal charges against him because I didn’t have enough evidence to have a case.\n",
            "Cleaned Text: october domestic violence awareness month domestic violence survivor still struggling even four year lately feeling angry angry abusive ex received real consequence action man abused manner physically sexually emotionally verbally financially etc granted restraining order renewed year later unable press criminal charge didnt enough evidence case\n",
            "--------------------------------------------------\n",
            "Cleaned Text 1 Remaining Stopwords: []\n",
            "Cleaned Text 2 Remaining Stopwords: []\n",
            "Cleaned Text 3 Remaining Stopwords: []\n",
            "Cleaned Text 4 Remaining Stopwords: []\n",
            "Cleaned Text 5 Remaining Stopwords: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define max_words\n",
        "max_words = 600\n",
        "\n",
        "# Continue with tokenization and other preprocessing steps\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(cleaned_texts)\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 150\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "jQzx3KmUHAFI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts = ' '.join(stress['text']).split()\n",
        "word_series = pd.Series(cleaned_texts)\n",
        "word_counts = word_series.value_counts()\n",
        "\n",
        "# Display the top N most frequent words\n",
        "print(word_counts.head(50))\n",
        "\n",
        "# Analyze the cumulative distribution\n",
        "cumulative_distribution = word_counts.cumsum() / word_counts.sum()\n",
        "print(cumulative_distribution)\n",
        "max_words = cumulative_distribution[cumulative_distribution < 0.99].shape[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGTmuIGvHAgV",
        "outputId": "ce1922e1-0251-40eb-c021-656b5e99f277"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I          10513\n",
            "to          8163\n",
            "and         7669\n",
            "the         5775\n",
            "a           5217\n",
            "my          3938\n",
            "of          3583\n",
            "in          2676\n",
            "that        2641\n",
            "for         2417\n",
            "me          2328\n",
            "was         2241\n",
            "is          2069\n",
            "have        2053\n",
            "it          2009\n",
            "with        1996\n",
            "but         1804\n",
            "this        1520\n",
            "he          1433\n",
            "on          1418\n",
            "be          1286\n",
            "so          1226\n",
            "I'm         1201\n",
            "her         1174\n",
            "just        1167\n",
            "you         1155\n",
            "about       1151\n",
            "or          1141\n",
            "not         1128\n",
            "like        1104\n",
            "at          1081\n",
            "she         1064\n",
            "as           974\n",
            "out          902\n",
            "had          899\n",
            "if           880\n",
            "because      856\n",
            "been         840\n",
            "get          829\n",
            "up           826\n",
            "do           793\n",
            "what         780\n",
            "we           774\n",
            "know         765\n",
            "from         750\n",
            "feel         739\n",
            "would        737\n",
            "are          735\n",
            "when         716\n",
            "can          715\n",
            "dtype: int64\n",
            "I              0.043240\n",
            "to             0.076815\n",
            "and            0.108357\n",
            "the            0.132110\n",
            "a              0.153567\n",
            "                 ...   \n",
            "ruminated      0.999984\n",
            "(expensive)    0.999988\n",
            "condo          0.999992\n",
            "regrets        0.999996\n",
            "event)         1.000000\n",
            "Length: 22146, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "vslHwC-2IGMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(padded_sequences, labels, test_size=0.2, random_state=36)\n",
        "val_data, test_data, val_labels, test_labels = train_test_split(test_data, test_labels, test_size=0.5, random_state=36)"
      ],
      "metadata": {
        "id": "n_ebA6PEHFVM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "VB3w9eGlIveY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Model - v1\n",
        "embedding_dim = 64\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "    layers.SpatialDropout1D(0.3),\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.5)),\n",
        "    layers.Bidirectional(layers.LSTM(32)),\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Use Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "# Train and test\n",
        "history = model.fit(train_data, train_labels, epochs=20, batch_size=75, validation_data=(val_data, val_labels), callbacks=[early_stopping])\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nonVmtYgIHkT",
        "outputId": "cd2e7c6d-2227-42da-f34a-e9319ae3029d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "31/31 [==============================] - 28s 580ms/step - loss: 0.7560 - accuracy: 0.4965 - val_loss: 0.7546 - val_accuracy: 0.5141\n",
            "Epoch 2/20\n",
            "31/31 [==============================] - 17s 566ms/step - loss: 0.7535 - accuracy: 0.5110 - val_loss: 0.7522 - val_accuracy: 0.5035\n",
            "Epoch 3/20\n",
            "31/31 [==============================] - 15s 496ms/step - loss: 0.7506 - accuracy: 0.5273 - val_loss: 0.7499 - val_accuracy: 0.5035\n",
            "Epoch 4/20\n",
            "31/31 [==============================] - 21s 689ms/step - loss: 0.7477 - accuracy: 0.5295 - val_loss: 0.7475 - val_accuracy: 0.5035\n",
            "Epoch 5/20\n",
            "31/31 [==============================] - 19s 605ms/step - loss: 0.7453 - accuracy: 0.5348 - val_loss: 0.7445 - val_accuracy: 0.5070\n",
            "Epoch 6/20\n",
            "31/31 [==============================] - 15s 498ms/step - loss: 0.7417 - accuracy: 0.5551 - val_loss: 0.7410 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "31/31 [==============================] - 19s 607ms/step - loss: 0.7372 - accuracy: 0.5828 - val_loss: 0.7355 - val_accuracy: 0.5739\n",
            "Epoch 8/20\n",
            "31/31 [==============================] - 15s 491ms/step - loss: 0.7277 - accuracy: 0.6145 - val_loss: 0.7236 - val_accuracy: 0.5704\n",
            "Epoch 9/20\n",
            "31/31 [==============================] - 18s 599ms/step - loss: 0.7081 - accuracy: 0.6392 - val_loss: 0.7142 - val_accuracy: 0.5951\n",
            "Epoch 10/20\n",
            "31/31 [==============================] - 16s 517ms/step - loss: 0.6830 - accuracy: 0.6419 - val_loss: 0.6850 - val_accuracy: 0.6092\n",
            "Epoch 11/20\n",
            "31/31 [==============================] - 15s 468ms/step - loss: 0.6572 - accuracy: 0.6599 - val_loss: 0.6809 - val_accuracy: 0.6373\n",
            "Epoch 12/20\n",
            "31/31 [==============================] - 15s 470ms/step - loss: 0.6194 - accuracy: 0.7057 - val_loss: 0.6682 - val_accuracy: 0.6549\n",
            "Epoch 13/20\n",
            "31/31 [==============================] - 14s 451ms/step - loss: 0.6015 - accuracy: 0.7269 - val_loss: 0.6622 - val_accuracy: 0.6514\n",
            "Epoch 14/20\n",
            "31/31 [==============================] - 14s 454ms/step - loss: 0.5789 - accuracy: 0.7419 - val_loss: 0.6613 - val_accuracy: 0.6585\n",
            "Epoch 15/20\n",
            "31/31 [==============================] - 14s 460ms/step - loss: 0.5660 - accuracy: 0.7476 - val_loss: 0.6543 - val_accuracy: 0.6514\n",
            "Epoch 16/20\n",
            "31/31 [==============================] - 15s 480ms/step - loss: 0.5549 - accuracy: 0.7586 - val_loss: 0.6543 - val_accuracy: 0.6761\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 0.5845 - accuracy: 0.7148\n",
            "Test accuracy: 0.7147887349128723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Model - v2\n",
        "embedding_dim = 64\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "    layers.SpatialDropout1D(0.2),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.4)),\n",
        "    layers.Bidirectional(layers.LSTM(16)),\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Use Adam optimizer with gradient clipping\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train and test\n",
        "history = model.fit(train_data, train_labels, epochs=20, batch_size=50, validation_data=(val_data, val_labels), callbacks=[early_stopping])\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kyiaARSIKRf",
        "outputId": "47ddb367-93b1-4cf8-ea9b-c3cd27827754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "46/46 [==============================] - 24s 339ms/step - loss: 0.7251 - accuracy: 0.5026 - val_loss: 0.7242 - val_accuracy: 0.5035\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 13s 270ms/step - loss: 0.7227 - accuracy: 0.5317 - val_loss: 0.7226 - val_accuracy: 0.5035\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 13s 281ms/step - loss: 0.7208 - accuracy: 0.5352 - val_loss: 0.7210 - val_accuracy: 0.5035\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 13s 271ms/step - loss: 0.7183 - accuracy: 0.5388 - val_loss: 0.7192 - val_accuracy: 0.5070\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 13s 268ms/step - loss: 0.7161 - accuracy: 0.5366 - val_loss: 0.7166 - val_accuracy: 0.5176\n",
            "Epoch 6/20\n",
            "36/46 [======================>.......] - ETA: 2s - loss: 0.7134 - accuracy: 0.5578"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Model - v3\n",
        "embedding_dim = 50\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "    layers.SpatialDropout1D(0.4),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.5)),\n",
        "    layers.Bidirectional(layers.LSTM(16)),\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.6),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Use Adam optimizer with a learning rate scheduler\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.0001 * 10**(epoch/20))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "# Train and test\n",
        "history = model.fit(train_data, train_labels, epochs=20, batch_size=40, validation_data=(val_data, val_labels), callbacks=[early_stopping, lr_schedule])\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "4nHieJN8INXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Model - v4\n",
        "embedding_dim = 50\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "    layers.SpatialDropout1D(0.5),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=True, dropout=0.6)),\n",
        "    layers.Bidirectional(layers.LSTM(16)),\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    layers.Dropout(0.7),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Use Adam optimizer with a learning rate scheduler\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.0001 * 10**(epoch/20))\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train and test\n",
        "history = model.fit(train_data, train_labels, epochs=20, batch_size=40, validation_data=(val_data, val_labels), callbacks=[early_stopping, lr_schedule, reduce_lr])\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndz9LU8sIT_R",
        "outputId": "e7b93a9a-d43c-490e-d1c5-9ed22b919cb7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 24s 285ms/step - loss: 1.0092 - accuracy: 0.5013 - val_loss: 1.0000 - val_accuracy: 0.5035 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 15s 265ms/step - loss: 0.9908 - accuracy: 0.5079 - val_loss: 0.9814 - val_accuracy: 0.5035 - lr: 1.1220e-04\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 15s 260ms/step - loss: 0.9719 - accuracy: 0.5084 - val_loss: 0.9621 - val_accuracy: 0.5035 - lr: 1.2589e-04\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 15s 257ms/step - loss: 0.9520 - accuracy: 0.5185 - val_loss: 0.9417 - val_accuracy: 0.5035 - lr: 1.4125e-04\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 14s 254ms/step - loss: 0.9305 - accuracy: 0.5194 - val_loss: 0.9204 - val_accuracy: 0.5035 - lr: 1.5849e-04\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 14s 255ms/step - loss: 0.9090 - accuracy: 0.5211 - val_loss: 0.8983 - val_accuracy: 0.5035 - lr: 1.7783e-04\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 18s 314ms/step - loss: 0.8862 - accuracy: 0.5304 - val_loss: 0.8757 - val_accuracy: 0.5035 - lr: 1.9953e-04\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 15s 255ms/step - loss: 0.8636 - accuracy: 0.5383 - val_loss: 0.8523 - val_accuracy: 0.5070 - lr: 2.2387e-04\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 15s 261ms/step - loss: 0.8389 - accuracy: 0.5520 - val_loss: 0.8238 - val_accuracy: 0.5951 - lr: 2.5119e-04\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 15s 253ms/step - loss: 0.8046 - accuracy: 0.5987 - val_loss: 0.7780 - val_accuracy: 0.6514 - lr: 2.8184e-04\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 14s 249ms/step - loss: 0.7511 - accuracy: 0.6445 - val_loss: 0.7275 - val_accuracy: 0.6725 - lr: 3.1623e-04\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 15s 262ms/step - loss: 0.7084 - accuracy: 0.6744 - val_loss: 0.6995 - val_accuracy: 0.6866 - lr: 3.5481e-04\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 15s 260ms/step - loss: 0.6603 - accuracy: 0.7238 - val_loss: 0.6936 - val_accuracy: 0.6514 - lr: 3.9811e-04\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 15s 259ms/step - loss: 0.6279 - accuracy: 0.7291 - val_loss: 0.6662 - val_accuracy: 0.6831 - lr: 4.4668e-04\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 15s 262ms/step - loss: 0.5965 - accuracy: 0.7392 - val_loss: 0.6642 - val_accuracy: 0.6725 - lr: 5.0119e-04\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 19s 342ms/step - loss: 0.5918 - accuracy: 0.7507 - val_loss: 0.6419 - val_accuracy: 0.6937 - lr: 5.6234e-04\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 15s 271ms/step - loss: 0.5622 - accuracy: 0.7626 - val_loss: 0.6348 - val_accuracy: 0.6690 - lr: 6.3096e-04\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 16s 270ms/step - loss: 0.5519 - accuracy: 0.7771 - val_loss: 0.6425 - val_accuracy: 0.6690 - lr: 7.0795e-04\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 16s 273ms/step - loss: 0.5217 - accuracy: 0.7846 - val_loss: 0.6274 - val_accuracy: 0.6972 - lr: 7.9433e-04\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 15s 268ms/step - loss: 0.5057 - accuracy: 0.7863 - val_loss: 0.6312 - val_accuracy: 0.6796 - lr: 8.9125e-04\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 0.5901 - accuracy: 0.6901\n",
            "Test accuracy: 0.6901408433914185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "__gr-krlIaFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT V1\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Convert data to InputExample format\n",
        "def convert_to_input_example(data, label):\n",
        "    input_examples = []\n",
        "    for text, label in zip(data, label):\n",
        "        input_examples.append(InputExample(guid=None, text_a=text, text_b=None, label=label))\n",
        "    return input_examples\n",
        "\n",
        "train_examples = convert_to_input_example(train_data, train_labels)\n",
        "val_examples = convert_to_input_example(val_data, val_labels)\n",
        "\n",
        "# Convert InputExamples to InputFeatures\n",
        "def convert_to_input_features(examples, max_length=128):\n",
        "    features = []\n",
        "    for example in examples:\n",
        "        # Extract the token IDs directly from the example\n",
        "        input_ids = example.text_a.tolist()\n",
        "\n",
        "        # Ensure the length is consistent with max_length\n",
        "        if len(input_ids) > max_length:\n",
        "            input_ids = input_ids[:max_length]\n",
        "        else:\n",
        "            input_ids += [0] * (max_length - len(input_ids))\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
        "\n",
        "        # Since BERT typically expects token_type_ids, we'll create a dummy one\n",
        "        token_type_ids = [0] * max_length\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=example.label\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "train_features = convert_to_input_features(train_examples)\n",
        "val_features = convert_to_input_features(val_examples)\n",
        "\n",
        "\n",
        "# Prepare dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": [f.input_ids for f in train_features], \"attention_mask\": [f.attention_mask for f in train_features]}, [f.label for f in train_features]))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": [f.input_ids for f in val_features], \"attention_mask\": [f.attention_mask for f in val_features]}, [f.label for f in val_features]))\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset.batch(32), validation_data=val_dataset.batch(32), epochs=3)\n",
        "# Print the results\n",
        " print(f\"Training   - Accuracy: {train_accuracy*100:.2f}% | Loss: {train_loss:.4f}\")\n",
        " print(f\"Validation - Accuracy: {val_accuracy*100:.2f}% | Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "e_XyuVh_IZxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert V2\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def convert_data_to_features(data, labels, max_length=128):\n",
        "    \"\"\"Convert data and labels into BERT's InputFeatures format.\"\"\"\n",
        "    # Convert data to InputExample format\n",
        "    examples = [InputExample(guid=None, text_a=text, text_b=None, label=label) for text, label in zip(data, labels)]\n",
        "\n",
        "    # Convert examples to InputFeatures\n",
        "    features = []\n",
        "    for example in examples:\n",
        "        input_ids = example.text_a.tolist()\n",
        "        input_ids = input_ids[:max_length] + [0] * (max_length - len(input_ids))\n",
        "        attention_mask = [1 if token_id != 0 else 0 for token_id in input_ids]\n",
        "        token_type_ids = [0] * max_length\n",
        "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=example.label))\n",
        "\n",
        "    return features\n",
        "\n",
        "train_features = convert_data_to_features(train_data, train_labels)\n",
        "val_features = convert_data_to_features(val_data, val_labels)\n",
        "\n",
        "# Prepare dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": [f.input_ids for f in train_features], \"attention_mask\": [f.attention_mask for f in train_features]}, [f.label for f in train_features]))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": [f.input_ids for f in val_features], \"attention_mask\": [f.attention_mask for f in val_features]}, [f.label for f in val_features]))\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset.batch(32), validation_data=val_dataset.batch(32), epochs=3)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Train the model for one epoch\n",
        "    train_history = model.fit(train_dataset.batch(32), verbose=0)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy = model.evaluate(val_dataset.batch(32), verbose=0)\n",
        "\n",
        "    # Extract training accuracy and loss from the history\n",
        "    train_accuracy = train_history.history['accuracy'][0]\n",
        "    train_loss = train_history.history['loss'][0]\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Training   - Accuracy: {train_accuracy*100:.2f}% | Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation - Accuracy: {val_accuracy*100:.2f}% | Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "YHQXjDc4Ide1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}